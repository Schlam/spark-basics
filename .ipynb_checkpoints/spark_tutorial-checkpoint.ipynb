{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "**Analyze large datasets on clusters using Apache Spark**\n",
    "\n",
    "This notebook is intended to run in Google CoLab.\n",
    "Skip the next code block if ran locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T20:51:28.902749Z",
     "start_time": "2020-12-31T20:51:28.898244Z"
    }
   },
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest version of spark from http://www-us.apache.org/dist/spark/ \n",
    "spark_version = 'spark-3.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ['SPARK_VERSION'] = spark_version\n",
    "os.environ['BASE_URL'] = 'http://www-us.apache.org/dist/spark'\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Install spark, java, and findspark\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q $BASE_URL/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Initialize spark session\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start session\n",
    "\n",
    "Start a spark session pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"DataFrameBasics\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame\n",
    "\n",
    "Using an array of tuples and two headers, create an example dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataframe\n",
    "df = spark.createDataFrame(\n",
    "    [(0, 'A'),\n",
    "     (1, 'B'), \n",
    "     (2, 'C'),], \n",
    "    ['id', 'words'])\n",
    "\n",
    "# Show head of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkFiles\n",
    "Connect to Amazon's S3 using sparkfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T18:53:01.216975Z",
     "start_time": "2020-12-31T18:53:01.212081Z"
    }
   },
   "source": [
    "#### Food data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "\n",
    "# Url for data in s3\n",
    "url = 'https://s3.amazonaws.com/datavix-curriculum/day_1/food.csv'\n",
    "\n",
    "# Add file to session\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# create dataframe from csv\n",
    "df = spark.read.csv(\n",
    "    SparkFiles.get('food.csv'), \n",
    "    header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T20:52:52.053215Z",
     "start_time": "2020-12-31T20:52:51.870240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Describe data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform data\n",
    "Change price data type to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T18:53:33.503167Z",
     "start_time": "2020-12-31T18:53:33.498375Z"
    }
   },
   "source": [
    "##### Change schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField as Field\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType\n",
    "\n",
    "\n",
    "# Create a list of structure fields\n",
    "schema = StructType(fields=[\n",
    "    Field(\"food\", StringType(), True), \n",
    "    Field(\"price\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Load dataframe with correct schema\n",
    "dataframe = spark.read.csv(\n",
    "    SparkFiles.get('food.csv'),\n",
    "    schema=schema,\n",
    "    header=True)\n",
    "\n",
    "# Print schema\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other data frame methods\n",
    "\n",
    "Manipulate columns using the ```.withColumn()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New price column\n",
    "dataframe.withColumn(\n",
    "    'newprice', dataframe['price']\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column name uppercase\n",
    "dataframe.withColumnRenamed('price','Price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use arithmetic function to transform the data\n",
    "dataframe.withColumn(\n",
    "    'DoublePrice', dataframe['Price'] * 2\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
